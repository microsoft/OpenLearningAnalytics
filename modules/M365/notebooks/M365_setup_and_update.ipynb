{
  "metadata": {
    "saveOutput": true,
    "kernelspec": {
      "name": "synapse_pyspark",
      "display_name": "Synapse PySpark"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## M365 module setup\n",
        "\n",
        "\n",
        "\n",
        "\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "outputs": [],
      "metadata": {
        "tags": [
          "parameters"
        ]
      },
      "source": [
        "storage_account = 'steduanalytics__update_this'\n",
        "use_test_env = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "outputs": [],
      "metadata": {},
      "source": [
        "if use_test_env:\n",
        "    stage1 = 'abfss://test-env@' + storage_account + '.dfs.core.windows.net/stage1'\n",
        "    stage2 = 'abfss://test-env@' + storage_account + '.dfs.core.windows.net/stage2'\n",
        "    stage3 = 'abfss://test-env@' + storage_account + '.dfs.core.windows.net/stage3'\n",
        "else:\n",
        "    stage1 = 'abfss://stage1@' + storage_account + '.dfs.core.windows.net'\n",
        "    stage2 = 'abfss://stage2@' + storage_account + '.dfs.core.windows.net'\n",
        "    stage3 = 'abfss://stage3@' + storage_account + '.dfs.core.windows.net'"
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "outputs": [],
      "metadata": {},
      "source": [
        "stage1_m365 = stage1 + '/m365/DIPData'\n",
        "stage1_m365_activity = stage1 + '/m365/DIPData/Activity/ApplicationUsage'\n",
        "\n",
        "# Process Roster data from stage 1 to stage 2\n",
        "#\n",
        "# Sets up the edu_dl (stage 2 data lake) with whatever data is found in the DIP inbound folder.\n",
        "# This includes:\n",
        "# - adding column names\n",
        "# - casting values into a schema\n",
        "\n",
        "# Calendar\n",
        "df = spark.read.csv(stage1_m365 + '/Roster/Calendar.csv', header='false')\n",
        "if (df.count() > 0):\n",
        "  sqlContext.registerDataFrameAsTable(df, 'Calendar')\n",
        "  df = spark.sql(\"select _c0 Id, _c1 Name, _c2 Description, cast(_c3 as int) SchoolYear, cast(_c4 as boolean) IsCurrent, _c5 ExternalId, to_timestamp(_c6, 'MM/dd/yyyy hh:mm:ss a') CreateDate, to_timestamp(_c7, 'MM/dd/yyyy hh:mm:ss a') LastModifiedDate, cast(_c8 as boolean) IsActive, _c9 OrgId from Calendar\")\n",
        "  df.write.format(\"parquet\").mode(\"overwrite\").save(stage2 + '/m365/Calendar')\n",
        "# Course\n",
        "df = spark.read.csv(stage1_m365 + '/Roster/Course.csv', header='false')\n",
        "if (df.count() > 0):\n",
        "  sqlContext.registerDataFrameAsTable(df, 'Course')\n",
        "  df = spark.sql(\"select _c0 Id, _c1 Name, _c2 Code, _c3 Description, _c4 ExternalId, to_timestamp(_c5, 'MM/dd/yyyy hh:mm:ss a') CreateDate, to_timestamp(_c6, 'MM/dd/yyyy hh:mm:ss a') LastModifiedDate, cast(_c7 as boolean) IsActive, _c8 CalendarId from Course\")\n",
        "  df.write.format(\"parquet\").mode(\"overwrite\").save(stage2 + '/m365/Course')\n",
        "# Org\n",
        "df = spark.read.csv(stage1_m365 + '/Roster/Org.csv', header='false')\n",
        "if (df.count() > 0):\n",
        "  sqlContext.registerDataFrameAsTable(df, 'Org')\n",
        "  df = spark.sql(\"select _c0 Id, _c1 Name, _c2 Identifier, _c3 ExternalId, to_timestamp(_c4, 'MM/dd/yyyy hh:mm:ss a') CreateDate, to_timestamp(_c5, 'MM/dd/yyyy hh:mm:ss a') LastModifiedDate, cast(_c6 as boolean) IsActive, _c7 ParentOrgId, _c8 RefOrgTypeId, _c9 SourceSystemId from Org\")\n",
        "  df.write.format('parquet').mode('overwrite').save(stage2 + '/m365/Org')\n",
        "# Person\n",
        "df = spark.read.csv(stage1_m365 + '/Roster/Person.csv', header='false')\n",
        "if (df.count() > 0):\n",
        "  sqlContext.registerDataFrameAsTable(df, 'Person')\n",
        "  df_Person = spark.sql(\"select _c0 Id, _c1 FirstName, _c2 MiddleName, _c3 LastName, _c4 GenerationCode, _c5 Prefix, _c6 EnabledUser, _c7 ExternalId, to_timestamp(_c8, 'MM/dd/yyyy hh:mm:ss a') CreateDate, to_timestamp(_c9, 'MM/dd/yyyy hh:mm:ss a') LastModifiedDate, cast(_c10 as boolean) IsActive, _c11 SourceSystemId from Person\")\n",
        "  df_Person.write.format('parquet').mode('overwrite').save(stage2 + '/m365/Person')\n",
        "# PersonIdentifier\n",
        "df = spark.read.csv(stage1_m365 + '/Roster/PersonIdentifier.csv', header='false')\n",
        "if (df.count() > 0):\n",
        "  sqlContext.registerDataFrameAsTable(df, 'PersonIdentifier')\n",
        "  df = spark.sql(\"select _c0 Id, _c1 Identifier, _c2 Description, _c3 RefIdentifierTypeId, _c4 ExternalId, to_timestamp(_c5, 'MM/dd/yyyy hh:mm:ss a') CreateDate, to_timestamp(_c6, 'MM/dd/yyyy hh:mm:ss a') LastModifiedDate, cast(_c7 as boolean) IsActive, _c8 PersonId, _c9 SourceSystemId from PersonIdentifier\")\n",
        "  df.write.format('parquet').mode('overwrite').save(stage2 + '/m365/PersonIdentifier')\n",
        "# RefDefinition\n",
        "df = spark.read.csv(stage1_m365 + '/Roster/RefDefinition.csv', header='false')\n",
        "if (df.count() > 0):\n",
        "  sqlContext.registerDataFrameAsTable(df, 'RefDefinition')\n",
        "  df = spark.sql(\"select _c0 Id, _c1 RefType, _c2 Namespace, _c3 Code, cast(_c4 as int) SortOrder, _c5 Description, cast(_c6 as boolean) IsActive from RefDefinition\")\n",
        "  df.write.format('parquet').mode('overwrite').save(stage2 + '/m365/RefDefinition')\n",
        "# Section\n",
        "df = spark.read.csv(stage1_m365 + '/Roster/Section.csv', header='false')\n",
        "if (df.count() > 0):\n",
        "  sqlContext.registerDataFrameAsTable(df, 'Section')\n",
        "  df = spark.sql(\"select _c0 Id, _c1 Name, _c2 Code, _c3 Location, _c4 ExternalId, to_timestamp(_c5, 'MM/dd/yyyy hh:mm:ss a') CreateDate, to_timestamp(_c6, 'MM/dd/yyyy hh:mm:ss a') LastModifiedDate, cast(_c7 as boolean) IsActive, _c8 CourseId, _c9 RefSectionTypeId, _c10 SessionId, _c11 OrgId from Section\")\n",
        "  df.write.format('parquet').mode('overwrite').save(stage2 + '/m365/Section')\n",
        "# Session\n",
        "df = spark.read.csv(stage1_m365 + '/Roster/Session.csv', header='false')\n",
        "if (df.count() > 0):\n",
        "  sqlContext.registerDataFrameAsTable(df, 'Session')\n",
        "  df = spark.sql(\"select _c0 Id, _c1 Name, to_timestamp(_c2, 'MM/dd/yyyy hh:mm:ss a') BeginDate, to_timestamp(_c3, 'MM/dd/yyyy hh:mm:ss a') EndDate, _c4 ExternalId, to_timestamp(_c5, 'MM/dd/yyyy hh:mm:ss a') CreateDate, to_timestamp(_c6, 'MM/dd/yyyy hh:mm:ss a') LastModifiedDate, cast(_c7 as boolean) IsActive, _c8 CalendarId, _c9 ParentSessionId, _c10 RefSessionTypeId from Session\")\n",
        "  df.write.format('parquet').mode('overwrite').save(stage2 + '/m365/Session')\n",
        "# StaffOrgAffiliation\n",
        "df = spark.read.csv(stage1_m365 + '/Roster/StaffOrgAffiliation.csv', header='false')\n",
        "if (df.count() > 0):\n",
        "  sqlContext.registerDataFrameAsTable(df, 'StaffOrgAffiliation')\n",
        "  df = spark.sql(\"select _c0 Id, cast(_c1 as boolean) IsPrimary, to_timestamp(_c2, 'MM/dd/yyyy hh:mm:ss a') EntryDate, to_timestamp(_c3, 'MM/dd/yyyy hh:mm:ss a') ExitDate, _c4 ExternalId, to_timestamp(_c5, 'MM/dd/yyyy hh:mm:ss a') CreateDate, to_timestamp(_c6, 'MM/dd/yyyy hh:mm:ss a') LastModifiedDate, cast(_c7 as boolean) IsActive, _c8 OrgId, _c9 PersonId, _c10 RefStaffOrgRoleId from StaffOrgAffiliation\")\n",
        "  df.write.format('parquet').mode('overwrite').save(stage2 + '/m365/StaffOrgAffiliation')\n",
        "# StaffSectionMembership\n",
        "df = spark.read.csv(stage1_m365 + '/Roster/StaffSectionMembership.csv', header='false')\n",
        "if (df.count() > 0):\n",
        "  sqlContext.registerDataFrameAsTable(df, 'StaffSectionMembership')\n",
        "  df = spark.sql(\"select _c0 Id, cast(_c1 as boolean) IsPrimaryStaffForSection, to_timestamp(_c2, 'MM/dd/yyyy hh:mm:ss a') EntryDate, to_timestamp(_c3, 'MM/dd/yyyy hh:mm:ss a') ExitDate, _c4 ExternalId, to_timestamp(_c5, 'MM/dd/yyyy hh:mm:ss a') CreateDate, to_timestamp(_c6, 'MM/dd/yyyy hh:mm:ss a') LastModifiedDate, cast(_c7 as boolean) IsActive, _c8 PersonId, _c9 RefStaffSectionRoleId, _c10 SectionId from StaffSectionMembership\")\n",
        "  df.write.format('parquet').mode('overwrite').save(stage2 + '/m365/StaffSectionMembership')\n",
        "# StudentOrgAffiliation\n",
        "df = spark.read.csv(stage1_m365 + '/Roster/StudentOrgAffiliation.csv', header='false')\n",
        "if (df.count() > 0):\n",
        "  sqlContext.registerDataFrameAsTable(df, 'StudentOrgAffiliation')\n",
        "  df = spark.sql(\"select _c0 Id, cast(_c1 as boolean) IsPrimary, to_timestamp(_c2, 'MM/dd/yyyy hh:mm:ss a') EntryDate, to_timestamp(_c3, 'MM/dd/yyyy hh:mm:ss a') ExitDate, _c4 ExternalId, to_timestamp(_c5, 'MM/dd/yyyy hh:mm:ss a') CreateDate, to_timestamp(_c6, 'MM/dd/yyyy hh:mm:ss a') LastModifiedDate, cast(_c7 as boolean) IsActive, _c8 OrgId, _c9 PersonId, _c10 RefGradeLevelId, _c11 RefStudentOrgRoleId, _c12 RefEnrollmentStatusId from StudentOrgAffiliation\")\n",
        "  df.write.format('parquet').mode('overwrite').save(stage2 + '/m365/StudentOrgAffiliation')\n",
        "# StudentSectionMembership\n",
        "df = spark.read.csv(stage1_m365 + '/Roster/StudentSectionMembership.csv', header='false')\n",
        "if (df.count() > 0):\n",
        "  sqlContext.registerDataFrameAsTable(df, 'StudentSectionMembership')\n",
        "  df = spark.sql(\"select _c0 Id, to_timestamp(_c1, 'MM/dd/yyyy hh:mm:ss a') EntryDate, to_timestamp(_c2, 'MM/dd/yyyy hh:mm:ss a') ExitDate, _c3 ExternalId, to_timestamp(_c4, 'MM/dd/yyyy hh:mm:ss a') CreateDate, to_timestamp(_c5, 'MM/dd/yyyy hh:mm:ss a') LastModifiedDate, cast(_c6 as boolean) IsActive, _c7 PersonId, _c8 RefGradeLevelWhenCourseTakenId, _c9 RefStudentSectionRoleId, _c10 SectionId from StudentSectionMembership\")\n",
        "  df.write.format('parquet').mode('overwrite').save(stage2 + '/m365/StudentSectionMembership')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": "2020-10-29 13:56:09\nNo new activity data to load"
          },
          "execution_count": 4,
          "metadata": {}
        }
      ],
      "metadata": {},
      "source": [
        "# Process Activity data from stage1 into stage2.\n",
        "#\n",
        "# If this is the first load, it loads all activity data.\n",
        "# If this is a subsequent load, it determines the max date currently stored and only loads data from after that date.\n",
        "\n",
        "def append_to_activity_table(max_date=False):\n",
        "    df = spark.read.csv(stage1_m365_activity, header='false') \n",
        "    sqlContext.registerDataFrameAsTable(df, 'Activity')\n",
        "    df_Activity = spark.sql(\"select to_timestamp(_c0) BinDate, _c1 Upn, _c2 UserId, _c3 Application, cast(_c4 as int) SumNumberOfSignals, _c5 Client, cast(_c6 as int) Duration, _c7 LearningActivity, '' PersonId from Activity\")\n",
        "    \n",
        "    if (max_date):\n",
        "        df_Activity = df_Activity.filter(df_Activity.BinDate > max_date)\n",
        "\n",
        "    if (df_Activity.count() == 0):\n",
        "        print('No new activity data to load')\n",
        "    else:\n",
        "        print('Adding activity data later than: ' + str(max_date))\n",
        "        # The assumption here is that there will always be data in these inbound files\n",
        "        sqlContext.registerDataFrameAsTable(df_Activity, 'Activity')\n",
        "        sqlContext.registerDataFrameAsTable(spark.read.format('parquet').load(stage2 + '/m365/PersonIdentifier'), 'PersonIdentifier')\n",
        "        sqlContext.registerDataFrameAsTable(spark.read.format('parquet').load(stage2 + '/m365/RefDefinition'), 'RefDefinition')\n",
        "\n",
        "        df1 = spark.sql( \\\n",
        "        \"select act.BinDate, act.Upn, act.UserId, act.Application, act.SumNumberOfSignals, act.Client, act.Duration, act.LearningActivity, pi.PersonId \\\n",
        "        from PersonIdentifier pi, RefDefinition rd, Activity act \\\n",
        "        where \\\n",
        "            pi.RefIdentifierTypeId = rd.Id \\\n",
        "            and rd.RefType = 'RefIdentifierType' \\\n",
        "            and rd.Code = 'username' \\\n",
        "            and pi.Identifier = act.Upn \\\n",
        "            and act.Upn <> '' \\\n",
        "        \")\n",
        "\n",
        "        df2 = spark.sql( \\\n",
        "        \"select act.BinDate, act.Upn, act.UserId, act.Application, act.SumNumberOfSignals, act.Client, act.Duration, act.LearningActivity, pi.PersonId \\\n",
        "        from PersonIdentifier pi, RefDefinition rd, Activity act\\\n",
        "        where \\\n",
        "            pi.RefIdentifierTypeId = rd.Id\\\n",
        "            and rd.RefType = 'RefIdentifierType'\\\n",
        "            and rd.Code = 'ActiveDirectoryId'\\\n",
        "            and pi.Identifier = act.UserId\\\n",
        "            and act.UserId is not null\\\n",
        "        \")\n",
        "\n",
        "        df1.write.format(\"parquet\").mode(\"append\").save(stage2 + '/m365/Activity')\n",
        "        df2.write.format(\"parquet\").mode(\"append\").save(stage2 + '/m365/Activity')\n",
        "\n",
        "try:\n",
        "    df = spark.read.format('parquet').load(stage2 + '/m365/Activity')\n",
        "    sqlContext.registerDataFrameAsTable(df, 'Activity')\n",
        "    # Bad data with a date in the future can prevent the uploading of new activity data, therefore ensure that the watermark is calculated on good data by filtering with CURRENT_TIMESTAMP\n",
        "    df1 = spark.sql(\"select BinDate from Activity where BinDate < CURRENT_TIMESTAMP\")\n",
        "    max_date = df1.agg({'BinDate': 'max'}).first()[0]\n",
        "    print(max_date)\n",
        "    append_to_activity_table(max_date)    \n",
        "except:\n",
        "    print(\"No Activity data has been loaded into stage2 data lake yet.\")\n",
        "    append_to_activity_table()\n",
        "\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "outputs": [],
      "metadata": {},
      "source": [
        "# Anonymize the data from edu_dl (stage2) and load into anon_edu_dl (stage3)\n",
        "# - redact columns from Person table\n",
        "# - apply a hash to every occurrence of PersonId\n",
        "# - redact UPN and UserId from Activity table\n",
        "# - don't bring in PersonIdentifier, Student, Staff and other tables not needed or empty (some tables are not being populated by EDP)\n",
        "\n",
        "from pyspark.sql.functions import sha2, lit\n",
        "# Activity\n",
        "df = spark.read.format('parquet').load(stage2 + '/m365/Activity')\n",
        "df = df.withColumn('PersonId', sha2(df.PersonId, 256)).withColumn('Upn', lit('*')).withColumn('UserId', lit('*'))\n",
        "df.write.format('parquet').mode('overwrite').save(stage3 + '/m365/Activity')\n",
        "# Calendar, Course, Org\n",
        "spark.read.format('parquet').load(stage2 + '/m365/Calendar').write.format('parquet').mode('overwrite').save(stage3 + '/m365/Calendar')\n",
        "spark.read.format('parquet').load(stage2 + '/m365/Course').write.format('parquet').mode('overwrite').save(stage3 + '/m365/Course')\n",
        "spark.read.format('parquet').load(stage2 + '/m365/Org').write.format('parquet').mode('overwrite').save(stage3 + '/m365/Org')\n",
        "# Person\n",
        "df = spark.read.format('parquet').load(stage2 + '/m365/Person')\n",
        "df = df.withColumn('Id', sha2(df.Id, 256)).withColumn('FirstName', lit('*')).withColumn(\"MiddleName\", lit('*')).withColumn('LastName', lit('*')).withColumn('ExternalId', sha2(df.ExternalId, 256))\n",
        "df.write.format('parquet').mode('overwrite').save(stage3 + '/m365/Person')\n",
        "# PersonIdentifier\n",
        "df = spark.read.format('parquet').load(stage2 + '/m365/PersonIdentifier')\n",
        "df = df.withColumn('PersonId', sha2(df.Id, 256)).withColumn('Identifier', lit('*')).withColumn(\"ExternalId\", lit('*'))\n",
        "df.write.format('parquet').mode('overwrite').save(stage3 + '/m365/PersonIdentifier')\n",
        "# RefDefinition, Section, Session\n",
        "spark.read.format('parquet').load(stage2 + '/m365/RefDefinition').write.format('parquet').mode('overwrite').save(stage3 + '/m365/RefDefinition')\n",
        "spark.read.format('parquet').load(stage2 + '/m365/Section').write.format('parquet').mode('overwrite').save(stage3 + '/m365/Section')\n",
        "spark.read.format('parquet').load(stage2 + '/m365/Session').write.format('parquet').mode('overwrite').save(stage3 + '/m365/Session')\n",
        "# StaffOrgAffiliation\n",
        "df = spark.read.format('parquet').load(stage2 + '/m365/StaffOrgAffiliation')\n",
        "df = df.withColumn('PersonId', sha2(df.PersonId, 256)).withColumn('ExternalId', lit('*'))\n",
        "df.write.format('parquet').mode('overwrite').save(stage3 + '/m365/StaffOrgAffiliation')\n",
        "# StaffSectionMembership\n",
        "df = spark.read.format('parquet').load(stage2 + '/m365/StaffSectionMembership')\n",
        "df = df.withColumn('PersonId', sha2(df.PersonId, 256)).withColumn('ExternalId', lit('*'))\n",
        "df.write.format('parquet').mode('overwrite').save(stage3 + '/m365/StaffSectionMembership')\n",
        "# StudentOrgAffiliation\n",
        "df = spark.read.format('parquet').load(stage2 + '/m365/StudentOrgAffiliation')\n",
        "df = df.withColumn('PersonId', sha2(df.PersonId, 256)).withColumn('ExternalId', lit('*'))\n",
        "df.write.format('parquet').mode('overwrite').save(stage3 + '/m365/StudentOrgAffiliation')\n",
        "# StudentSectionMembership\n",
        "df = spark.read.format('parquet').load(stage2 + '/m365/StudentSectionMembership')\n",
        "df = df.withColumn('PersonId', sha2(df.PersonId, 256)).withColumn('ExternalId', lit('*'))\n",
        "df.write.format('parquet').mode('overwrite').save(stage3 + '/m365/StudentSectionMembership')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "outputs": [],
      "metadata": {},
      "source": [
        "# Create spark db to allow for access to the data in the delta-lake via SQL on-demand.\n",
        "# This is only creating metadata for SQL on-demand, pointing to the data in the delta-lake.\n",
        "# This also makes it possible to connect in Power BI via the azure sql data source connector.\n",
        "def create_spark_db(db_name, source_path):\n",
        "    spark.sql('CREATE DATABASE IF NOT EXISTS ' + db_name)\n",
        "    spark.sql(\"create table if not exists \" + db_name + \".Activity using PARQUET location '\" + source_path + \"/Activity'\")\n",
        "    spark.sql(\"create table if not exists \" + db_name + \".Calendar using PARQUET location '\" + source_path + \"/Calendar'\")\n",
        "    spark.sql(\"create table if not exists \" + db_name + \".Course using PARQUET location '\" + source_path + \"/Course'\")\n",
        "    spark.sql(\"create table if not exists \" + db_name + \".Org using PARQUET location '\" + source_path + \"/Org'\")\n",
        "    spark.sql(\"create table if not exists \" + db_name + \".Person using PARQUET location '\" + source_path + \"/Person'\")\n",
        "    spark.sql(\"create table if not exists \" + db_name + \".PersonIdentifier using PARQUET location '\" + source_path + \"/PersonIdentifier'\")\n",
        "    spark.sql(\"create table if not exists \" + db_name + \".RefDefinition using PARQUET location '\" + source_path + \"/RefDefinition'\")\n",
        "    spark.sql(\"create table if not exists \" + db_name + \".Section using PARQUET location '\" + source_path + \"/Section'\")\n",
        "    spark.sql(\"create table if not exists \" + db_name + \".Session using PARQUET location '\" + source_path + \"/Session'\")\n",
        "    spark.sql(\"create table if not exists \" + db_name + \".StaffOrgAffiliation using PARQUET location '\" + source_path + \"/StaffOrgAffiliation'\")\n",
        "    spark.sql(\"create table if not exists \" + db_name + \".StaffSectionMembership using PARQUET location '\" + source_path + \"/StaffSectionMembership'\")\n",
        "    spark.sql(\"create table if not exists \" + db_name + \".StudentOrgAffiliation using PARQUET location '\" + source_path + \"/StudentOrgAffiliation'\")\n",
        "    spark.sql(\"create table if not exists \" + db_name + \".StudentSectionMembership using PARQUET location '\" + source_path + \"/StudentSectionMembership'\")\n",
        "\n",
        "db_prefix = 'test_' if use_test_env else ''\n",
        "create_spark_db(db_prefix + 's2_m365', stage2 + '/m365')\n",
        "create_spark_db(db_prefix + 's3_m365', stage3 + '/m365')"
      ]
    }
  ]
}