{
  "metadata": {
    "saveOutput": true,
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup Clever data\n",
        "\n",
        "\n",
        "\n",
        "\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "tags": [
          "parameters"
        ]
      },
      "source": [
        "storage_account = 'steduanalytics__update_this'\n",
        "use_test_env = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "if use_test_env:\n",
        "    stage1 = 'abfss://test-env@' + storage_account + '.dfs.core.windows.net/stage1'\n",
        "    stage2 = 'abfss://test-env@' + storage_account + '.dfs.core.windows.net/stage2'\n",
        "    stage3 = 'abfss://test-env@' + storage_account + '.dfs.core.windows.net/stage3'\n",
        "else:\n",
        "    stage1 = 'abfss://stage1@' + storage_account + '.dfs.core.windows.net'\n",
        "    stage2 = 'abfss://stage2@' + storage_account + '.dfs.core.windows.net'\n",
        "    stage3 = 'abfss://stage3@' + storage_account + '.dfs.core.windows.net'"
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "# Process resource usage\n",
        "df = spark.read.csv(stage1 + '/clever', header='true', inferSchema='true')\n",
        "df = df.withColumn('sis_id',df.sis_id.cast('string'))\n",
        "df.write.format('parquet').mode('overwrite').save(stage2 + '/clever/resource_usage_students')\n",
        "\n",
        "# Anonymize data and load into stage3\n",
        "from pyspark.sql.functions import sha2, lit\n",
        "df = spark.read.format('parquet').load(stage2 + '/clever/resource_usage_students')\n",
        "df = df.withColumn('sis_id', sha2(df.sis_id, 256)).withColumn('clever_user_id',lit('*')).withColumn('clever_school_id',lit('*'))\n",
        "df.write.format('parquet').mode('overwrite').save(stage3 + '/clever/resource_usage_students')\n",
        "\n",
        "# Create sql on-demand db for Clever data\n",
        "def create_spark_db(db_name, source_path):\n",
        "    spark.sql('CREATE DATABASE IF NOT EXISTS ' + db_name)\n",
        "    spark.sql(\"create table if not exists \" + db_name + \".resource_usage_students using PARQUET location '\" + source_path + \"/resource_usage_students'\")\n",
        "\n",
        "db_prefix = 'test_' if use_test_env else ''\n",
        "create_spark_db(db_prefix + 's2_clever', stage2 + '/clever')\n",
        "create_spark_db(db_prefix + 's3_clever', stage3 + '/clever')"
      ],
      "attachments": {}
    }
  ]
}