{
  "metadata": {
    "saveOutput": true,
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Contoso ISD solution package\n",
        "This notebook is for creating a consolidated view over the data from each of the source systems.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "outputs": [],
      "metadata": {
        "tags": [
          "parameters"
        ]
      },
      "source": [
        "storage_account = 'steduanalytics__update_this'\n",
        "use_test_env = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "outputs": [],
      "metadata": {},
      "source": [
        "if use_test_env:\n",
        "    stage1 = 'abfss://test-env@' + storage_account + '.dfs.core.windows.net/stage1'\n",
        "    stage2 = 'abfss://test-env@' + storage_account + '.dfs.core.windows.net/stage2'\n",
        "    stage3 = 'abfss://test-env@' + storage_account + '.dfs.core.windows.net/stage3'\n",
        "else:\n",
        "    stage1 = 'abfss://stage1@' + storage_account + '.dfs.core.windows.net'\n",
        "    stage2 = 'abfss://stage2@' + storage_account + '.dfs.core.windows.net'\n",
        "    stage3 = 'abfss://stage3@' + storage_account + '.dfs.core.windows.net'"
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "outputs": [],
      "metadata": {},
      "source": [
        "# Process sectionmark data\n",
        "# Convert id values to use the Person.Id and Section.Id values set in the Education Data Platform.\n",
        "from pyspark.sql.functions import sha2, lit\n",
        "\n",
        "sqlContext.registerDataFrameAsTable(spark.read.format('parquet').load(stage2 + '/contoso_sis/studentsectionmark'), 'SectionMark')\n",
        "\n",
        "sqlContext.registerDataFrameAsTable(spark.read.format('parquet').load(stage2 + '/m365/Person'), 'Person')\n",
        "sqlContext.registerDataFrameAsTable(spark.read.format('parquet').load(stage2 + '/m365/Section'), 'Section')\n",
        "\n",
        "df = spark.sql(\"select sm.id Id, p.Id PersonId, s.Id SectionId, cast(sm.numeric_grade_earned as int) NumericGrade, \\\n",
        "sm.alpha_grade_earned AlphaGrade, sm.is_final_grade IsFinalGrade, cast(sm.credits_attempted as int) CreditsAttempted, cast(sm.credits_earned as int) CreditsEarned, \\\n",
        "sm.grad_credit_type GraduationCreditType, sm.id ExternalId, CURRENT_TIMESTAMP CreateDate, CURRENT_TIMESTAMP LastModifiedDate, true IsActive \\\n",
        "from SectionMark sm, Person p, Section s \\\n",
        "where sm.student_id = p.ExternalId \\\n",
        "and sm.section_id = s.ExternalId\")\n",
        "\n",
        "df.write.format('parquet').mode('overwrite').save(stage2 + '/ContosoISD/SectionMark')\n",
        "df.write.format('parquet').mode('overwrite').save(stage2 + '/ContosoISD/SectionMark2')\n",
        "# Add SectionMark data to stage3 (anonymized parquet lake)\n",
        "df = df.withColumn('PersonId', sha2(df.PersonId, 256))\n",
        "df.write.format('parquet').mode('overwrite').save(stage3 + '/ContosoISD/SectionMark')\n",
        "df.write.format('parquet').mode('overwrite').save(stage3 + '/ContosoISD/SectionMark2')\n",
        "\n",
        "# Repeat the above process, this time for student attendance\n",
        "# Convert id values to use the Person.Id, Org.Id and Section.Id values\n",
        "sqlContext.registerDataFrameAsTable(spark.read.format('parquet').load(stage2 + '/contoso_sis/studentattendance'), 'Attendance')\n",
        "sqlContext.registerDataFrameAsTable(spark.read.format('parquet').load(stage2 + '/m365/Org'), 'Org')\n",
        "\n",
        "df = spark.sql(\"select att.id Id, p.Id PersonId, att.school_year SchoolYear, o.Id OrgId, to_date(att.attendance_date,'MM/dd/yyyy') AttendanceDate, \\\n",
        "att.all_day AllDay, att.Period Period, s.Id SectionId, att.AttendanceCode AttendanceCode, att.PresenceFlag PresenceFlag, \\\n",
        "att.attendance_status AttendanceStatus, att.attendance_type AttendanceType, att.attendance_sequence AttendanceSequence \\\n",
        "from Attendance att, Org o, Person p, Section s \\\n",
        "where att.student_id = p.ExternalId \\\n",
        "and att.school_id = o.ExternalId \\\n",
        "and att.section_id = s.ExternalId\")\n",
        "\n",
        "df.write.format('parquet').mode('overwrite').save(stage2 +'/ContosoISD/Attendance')\n",
        "# Add Attendance data to stage3 (anonymized parquet lake)\n",
        "df = df.withColumn('PersonId', sha2(df.PersonId, 256))\n",
        "df.write.format('parquet').mode('overwrite').save(stage3 + '/ContosoISD/Attendance')\n",
        "\n",
        "# Add 'Department' column to Course (hardcoded to \"Math\" for this Contoso example)\n",
        "sqlContext.registerDataFrameAsTable(spark.read.format('parquet').load(stage2 + '/m365/Course'), 'Course')\n",
        "df = spark.sql(\"select Id, Name, Code, Description, ExternalId, CreateDate, LastModifiedDate, IsActive, CalendarId, 'Math' Department from Course\")\n",
        "df.write.format('parquet').mode('overwrite').save(stage2 + '/ContosoISD/Course')\n",
        "df.write.format('parquet').mode('overwrite').save(stage3 + '/ContosoISD/Course')\n",
        "\n",
        "# Create spark db to allow for access to the data in the delta-lake via SQL on-demand.\n",
        "# This is only creating metadata for SQL on-demand, pointing to the data in the delta-lake.\n",
        "# This also makes it possible to connect in Power BI via the azure sql data source connector.\n",
        "def create_spark_db(db_name, source_path):\n",
        "    spark.sql('CREATE DATABASE IF NOT EXISTS ' + db_name)\n",
        "    spark.sql(f\"create table if not exists \" + db_name + \".Activity using PARQUET location '\" + source_path + \"/m365/Activity'\")\n",
        "    spark.sql(f\"create table if not exists \" + db_name + \".Calendar using PARQUET location '\" + source_path + \"/m365/Calendar'\")\n",
        "    spark.sql(f\"create table if not exists \" + db_name + \".Org using PARQUET location '\" + source_path + \"/m365/Org'\")\n",
        "    spark.sql(f\"create table if not exists \" + db_name + \".Person using PARQUET location '\" + source_path + \"/m365/Person'\")\n",
        "    spark.sql(f\"create table if not exists \" + db_name + \".PersonIdentifier using PARQUET location '\" + source_path + \"/m365/PersonIdentifier'\")\n",
        "    spark.sql(f\"create table if not exists \" + db_name + \".RefDefinition using PARQUET location '\" + source_path + \"/m365/RefDefinition'\")\n",
        "    spark.sql(f\"create table if not exists \" + db_name + \".Section using PARQUET location '\" + source_path + \"/m365/Section'\")\n",
        "    spark.sql(f\"create table if not exists \" + db_name + \".Session using PARQUET location '\" + source_path + \"/m365/Session'\")\n",
        "    spark.sql(f\"create table if not exists \" + db_name + \".StaffOrgAffiliation using PARQUET location '\" + source_path + \"/m365/StaffOrgAffiliation'\")\n",
        "    spark.sql(f\"create table if not exists \" + db_name + \".StaffSectionMembership using PARQUET location '\" + source_path + \"/m365/StaffSectionMembership'\")\n",
        "    spark.sql(f\"create table if not exists \" + db_name + \".StudentOrgAffiliation using PARQUET location '\" + source_path + \"/m365/StudentOrgAffiliation'\")\n",
        "    spark.sql(f\"create table if not exists \" + db_name + \".StudentSectionMembership using PARQUET location '\" + source_path + \"/m365/StudentSectionMembership'\")\n",
        "\n",
        "    spark.sql(f\"create table if not exists \" + db_name + \".Course using PARQUET location '\" + source_path + \"/ContosoISD/Course'\")\n",
        "    spark.sql(f\"create table if not exists \" + db_name + \".Attendance using PARQUET location '\" + source_path + \"/ContosoISD/Attendance'\")\n",
        "    spark.sql(f\"create table if not exists \" + db_name + \".SectionMark using PARQUET location '\" + source_path + \"/ContosoISD/SectionMark'\")\n",
        "    spark.sql(f\"create table if not exists \" + db_name + \".SectionMark2 using PARQUET location '\" + source_path + \"/ContosoISD/SectionMark2'\")\n",
        "\n",
        "db_prefix = 'test_' if use_test_env else ''\n",
        "create_spark_db(db_prefix + 's2_ContosoISD', stage2)\n",
        "create_spark_db(db_prefix + 's3_ContosoISD', stage3)"
      ]
    }
  ]
}