{
  "metadata": {
    "saveOutput": true,
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## All of the module notebooks combined into this single notebook.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "outputs": [],
      "metadata": {
        "tags": [
          "parameters"
        ]
      },
      "source": [
        "storage_account = 'steduanalytics__update_this'\n",
        "use_test_env = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "outputs": [],
      "metadata": {},
      "source": [
        "if use_test_env:\n",
        "    stage1 = 'abfss://test-env@' + storage_account + '.dfs.core.windows.net/stage1'\n",
        "    stage2 = 'abfss://test-env@' + storage_account + '.dfs.core.windows.net/stage2'\n",
        "    stage3 = 'abfss://test-env@' + storage_account + '.dfs.core.windows.net/stage3'\n",
        "else:\n",
        "    stage1 = 'abfss://stage1@' + storage_account + '.dfs.core.windows.net'\n",
        "    stage2 = 'abfss://stage2@' + storage_account + '.dfs.core.windows.net'\n",
        "    stage3 = 'abfss://stage3@' + storage_account + '.dfs.core.windows.net'\n",
        "\n",
        ""
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "outputs": [],
      "metadata": {},
      "source": [
        "# Extracted from Clever_setup_and_update\n",
        "\n",
        "# Process resource usage\n",
        "df = spark.read.csv(stage1 + '/clever', header='true', inferSchema='true')\n",
        "df = df.withColumn('sis_id',df.sis_id.cast('string'))\n",
        "df.write.format('parquet').mode('overwrite').save(stage2 + '/clever/resource_usage_students')\n",
        "\n",
        "# Anonymize data and load into stage3\n",
        "from pyspark.sql.functions import sha2, lit\n",
        "df = spark.read.format('parquet').load(stage2 + '/clever/resource_usage_students')\n",
        "df = df.withColumn('sis_id', sha2(df.sis_id, 256)).withColumn('clever_user_id',lit('*')).withColumn('clever_school_id',lit('*'))\n",
        "df.write.format('parquet').mode('overwrite').save(stage3 + '/clever/resource_usage_students')\n",
        "\n",
        "# Create sql on-demand db for Clever data\n",
        "def create_spark_db(db_name, source_path):\n",
        "    spark.sql('CREATE DATABASE IF NOT EXISTS ' + db_name)\n",
        "    spark.sql(\"create table if not exists \" + db_name + \".resource_usage_students using PARQUET location '\" + source_path + \"/resource_usage_students'\")\n",
        "\n",
        "db_prefix = 'test_' if use_test_env else ''\n",
        "create_spark_db(db_prefix + 's2_clever', stage2 + '/clever')\n",
        "create_spark_db(db_prefix + 's3_clever', stage3 + '/clever')"
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "outputs": [],
      "metadata": {},
      "source": [
        "# Extracted from contoso_sis_setup_and_update\n",
        "\n",
        "# Process studentsectionmark and studentattendance\n",
        "df = spark.read.csv(stage1 + '/contoso_sis/studentsectionmark.csv', header='true', inferSchema='true')\n",
        "df = df.withColumn('id',df.id.cast('string')).withColumn('student_id',df.student_id.cast('string'))\n",
        "df.write.format('parquet').mode('overwrite').save(stage2 + '/contoso_sis/studentsectionmark')\n",
        "\n",
        "df = spark.read.csv(stage1 + '/contoso_sis/studentattendance.csv', header='true', inferSchema='true')\n",
        "df = df.withColumn('id',df.id.cast('string')).withColumn('student_id',df.student_id.cast('string'))\n",
        "df.write.format('parquet').mode('overwrite').save(stage2 + '/contoso_sis/studentattendance')\n",
        "\n",
        "# Anonymize data and load into stage3\n",
        "df = spark.read.format('parquet').load(stage2 + '/contoso_sis/studentsectionmark')\n",
        "df = df.withColumn('id', sha2(df.id, 256)).withColumn('student_id',sha2(df.student_id, 256))\n",
        "df.write.format('parquet').mode('overwrite').save(stage3 + '/contoso_sis/studentsectionmark')\n",
        "\n",
        "df = spark.read.format('parquet').load(stage2 + '/contoso_sis/studentattendance')\n",
        "df = df.withColumn('id', sha2(df.id, 256)).withColumn('student_id',sha2(df.student_id, 256))\n",
        "df.write.format('parquet').mode('overwrite').save(stage3 + '/contoso_sis/studentattendance')\n",
        "\n",
        "# Create spark db to allow for access to the data in the delta-lake via SQL on-demand.\n",
        "def create_spark_db(db_name, source_path):\n",
        "    spark.sql('CREATE DATABASE IF NOT EXISTS ' + db_name)\n",
        "    spark.sql(\"create table if not exists \" + db_name + \".studentsectionmark using PARQUET location '\" + source_path + \"/studentsectionmark'\")\n",
        "    spark.sql(\"create table if not exists \" + db_name + \".studentattendance using PARQUET location '\" + source_path + \"/studentattendance'\")\n",
        "\n",
        "db_prefix = 'test_' if use_test_env else ''\n",
        "create_spark_db(db_prefix + 's2_contoso_sis', stage2 + '/contoso_sis')\n",
        "create_spark_db(db_prefix + 's3_contoso_sis', stage3 + '/contoso_sis')"
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "outputs": [],
      "metadata": {},
      "source": [
        "# Extracted from iReady_setup_and_update\n",
        "\n",
        "# Process personalized_instruction_by_lesson_math.csv\n",
        "def remove_spaces(str): return str.replace(' ', '').replace('(','_').replace(')','_').replace('=', '__')\n",
        "\n",
        "def process(filename):\n",
        "  df = spark.read.csv(stage1 + '/iready/' + filename + '.csv', header='true', inferSchema='true')\n",
        "  newColumns = map(remove_spaces, df.columns)\n",
        "  df = df.toDF(*newColumns)\n",
        "  df = df.withColumn('StudentID',df.StudentID.cast('string')) # StudentID needs to be a string to allow for hashing when moving into stage3\n",
        "  df.write.format('parquet').mode('overwrite').save(stage2 + '/iready/' + filename)\n",
        "\n",
        "process('comprehensive_student_lesson_activity_with_standards_ela')\n",
        "process('comprehensive_student_lesson_activity_with_standards_math')\n",
        "process('diagnostic_and_instruction_ela_ytd_window')\n",
        "process('diagnostic_and_instruction_math_ytd_window')\n",
        "process('diagnostic_results_ela')\n",
        "process('diagnostic_results_math')\n",
        "process('personalized_instruction_by_lesson_ela')\n",
        "process('personalized_instruction_by_lesson_math')\n",
        "\n",
        "# Anonymize data and load into stage3\n",
        "from pyspark.sql.functions import sha2, lit\n",
        "\n",
        "df = spark.read.format('parquet').load(stage2 + '/iready/comprehensive_student_lesson_activity_with_standards_ela')\n",
        "df = df.withColumn('StudentID', sha2(df.StudentID, 256)).withColumn('LastName',lit('*')).withColumn('FirstName',lit('*'))\n",
        "df.write.format('parquet').mode('overwrite').save(stage3 + '/iready/comprehensive_student_lesson_activity_with_standards_ela')\n",
        "\n",
        "df = spark.read.format('parquet').load(stage2 + '/iready/comprehensive_student_lesson_activity_with_standards_math')\n",
        "df = df.withColumn('StudentID', sha2(df.StudentID, 256)).withColumn('LastName',lit('*')).withColumn('FirstName',lit('*'))\n",
        "df.write.format('parquet').mode('overwrite').save(stage3 + '/iready/comprehensive_student_lesson_activity_with_standards_math')\n",
        "\n",
        "df = spark.read.format('parquet').load(stage2 + '/iready/diagnostic_and_instruction_ela_ytd_window')\n",
        "df = df.withColumn('StudentID', sha2(df.StudentID, 256)).withColumn('LastName',lit('*')).withColumn('FirstName',lit('*')).withColumn('UserName', lit('*'))\n",
        "df.write.format('parquet').mode('overwrite').save(stage3 + '/iready/diagnostic_and_instruction_ela_ytd_window')\n",
        "\n",
        "df = spark.read.format('parquet').load(stage2 + '/iready/diagnostic_and_instruction_math_ytd_window')\n",
        "df = df.withColumn('StudentID', sha2(df.StudentID, 256)).withColumn('LastName',lit('*')).withColumn('FirstName',lit('*')).withColumn('UserName', lit('*'))\n",
        "df.write.format('parquet').mode('overwrite').save(stage3 + '/iready/diagnostic_and_instruction_math_ytd_window')\n",
        "\n",
        "df = spark.read.format('parquet').load(stage2 + '/iready/diagnostic_results_ela')\n",
        "df = df.withColumn('StudentID', sha2(df.StudentID, 256)).withColumn('LastName',lit('*')).withColumn('FirstName',lit('*'))\n",
        "df.write.format('parquet').mode('overwrite').save(stage3 + '/iready/diagnostic_results_ela')\n",
        "\n",
        "df = spark.read.format('parquet').load(stage2 + '/iready/diagnostic_results_math')\n",
        "df = df.withColumn('StudentID', sha2(df.StudentID, 256)).withColumn('LastName',lit('*')).withColumn('FirstName',lit('*'))\n",
        "df.write.format('parquet').mode('overwrite').save(stage3 + '/iready/diagnostic_results_math')\n",
        "\n",
        "df = spark.read.format('parquet').load(stage2 + '/iready/personalized_instruction_by_lesson_ela')\n",
        "df = df.withColumn('StudentID', sha2(df.StudentID, 256)).withColumn('LastName',lit('*')).withColumn('FirstName',lit('*'))\n",
        "df.write.format('parquet').mode('overwrite').save(stage3 + '/iready/personalized_instruction_by_lesson_ela')\n",
        "\n",
        "df = spark.read.format('parquet').load(stage2 + '/iready/personalized_instruction_by_lesson_math')\n",
        "df = df.withColumn('StudentID', sha2(df.StudentID, 256)).withColumn('LastName',lit('*')).withColumn('FirstName',lit('*'))\n",
        "df.write.format('parquet').mode('overwrite').save(stage3 + '/iready/personalized_instruction_by_lesson_math')"
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 7,
          "data": {
            "text/plain": "No Activity data has been loaded into stage2 data lake yet.\nAdding activity data later than: False"
          },
          "metadata": {}
        }
      ],
      "metadata": {},
      "source": [
        "# Extracted from M365_setup_and_update\n",
        "\n",
        "stage1_m365 = stage1 + '/m365/DIPData'\n",
        "stage1_m365_activity = stage1 + '/m365/DIPData/Activity/ApplicationUsage'\n",
        "\n",
        "# Process Roster data from stage 1 to stage 2\n",
        "#\n",
        "# Sets up the edu_dl (stage 2 data lake) with whatever data is found in the DIP inbound folder.\n",
        "# This includes:\n",
        "# - adding column names\n",
        "# - casting values into a schema\n",
        "\n",
        "# Calendar\n",
        "df = spark.read.csv(stage1_m365 + '/Roster/Calendar.csv', header='false')\n",
        "if (df.count() > 0):\n",
        "  sqlContext.registerDataFrameAsTable(df, 'Calendar')\n",
        "  df = spark.sql(\"select _c0 Id, _c1 Name, _c2 Description, cast(_c3 as int) SchoolYear, cast(_c4 as boolean) IsCurrent, _c5 ExternalId, to_timestamp(_c6, 'MM/dd/yyyy hh:mm:ss a') CreateDate, to_timestamp(_c7, 'MM/dd/yyyy hh:mm:ss a') LastModifiedDate, cast(_c8 as boolean) IsActive, _c9 OrgId from Calendar\")\n",
        "  df.write.format(\"parquet\").mode(\"overwrite\").save(stage2 + '/m365/Calendar')\n",
        "# Course\n",
        "df = spark.read.csv(stage1_m365 + '/Roster/Course.csv', header='false')\n",
        "if (df.count() > 0):\n",
        "  sqlContext.registerDataFrameAsTable(df, 'Course')\n",
        "  df = spark.sql(\"select _c0 Id, _c1 Name, _c2 Code, _c3 Description, _c4 ExternalId, to_timestamp(_c5, 'MM/dd/yyyy hh:mm:ss a') CreateDate, to_timestamp(_c6, 'MM/dd/yyyy hh:mm:ss a') LastModifiedDate, cast(_c7 as boolean) IsActive, _c8 CalendarId from Course\")\n",
        "  df.write.format(\"parquet\").mode(\"overwrite\").save(stage2 + '/m365/Course')\n",
        "# Org\n",
        "df = spark.read.csv(stage1_m365 + '/Roster/Org.csv', header='false')\n",
        "if (df.count() > 0):\n",
        "  sqlContext.registerDataFrameAsTable(df, 'Org')\n",
        "  df = spark.sql(\"select _c0 Id, _c1 Name, _c2 Identifier, _c3 ExternalId, to_timestamp(_c4, 'MM/dd/yyyy hh:mm:ss a') CreateDate, to_timestamp(_c5, 'MM/dd/yyyy hh:mm:ss a') LastModifiedDate, cast(_c6 as boolean) IsActive, _c7 ParentOrgId, _c8 RefOrgTypeId, _c9 SourceSystemId from Org\")\n",
        "  df.write.format('parquet').mode('overwrite').save(stage2 + '/m365/Org')\n",
        "# Person\n",
        "df = spark.read.csv(stage1_m365 + '/Roster/Person.csv', header='false')\n",
        "if (df.count() > 0):\n",
        "  sqlContext.registerDataFrameAsTable(df, 'Person')\n",
        "  df_Person = spark.sql(\"select _c0 Id, _c1 FirstName, _c2 MiddleName, _c3 LastName, _c4 GenerationCode, _c5 Prefix, _c6 EnabledUser, _c7 ExternalId, to_timestamp(_c8, 'MM/dd/yyyy hh:mm:ss a') CreateDate, to_timestamp(_c9, 'MM/dd/yyyy hh:mm:ss a') LastModifiedDate, cast(_c10 as boolean) IsActive, _c11 SourceSystemId from Person\")\n",
        "  df_Person.write.format('parquet').mode('overwrite').save(stage2 + '/m365/Person')\n",
        "# PersonIdentifier\n",
        "df = spark.read.csv(stage1_m365 + '/Roster/PersonIdentifier.csv', header='false')\n",
        "if (df.count() > 0):\n",
        "  sqlContext.registerDataFrameAsTable(df, 'PersonIdentifier')\n",
        "  df = spark.sql(\"select _c0 Id, _c1 Identifier, _c2 Description, _c3 RefIdentifierTypeId, _c4 ExternalId, to_timestamp(_c5, 'MM/dd/yyyy hh:mm:ss a') CreateDate, to_timestamp(_c6, 'MM/dd/yyyy hh:mm:ss a') LastModifiedDate, cast(_c7 as boolean) IsActive, _c8 PersonId, _c9 SourceSystemId from PersonIdentifier\")\n",
        "  df.write.format('parquet').mode('overwrite').save(stage2 + '/m365/PersonIdentifier')\n",
        "# RefDefinition\n",
        "df = spark.read.csv(stage1_m365 + '/Roster/RefDefinition.csv', header='false')\n",
        "if (df.count() > 0):\n",
        "  sqlContext.registerDataFrameAsTable(df, 'RefDefinition')\n",
        "  df = spark.sql(\"select _c0 Id, _c1 RefType, _c2 Namespace, _c3 Code, cast(_c4 as int) SortOrder, _c5 Description, cast(_c6 as boolean) IsActive from RefDefinition\")\n",
        "  df.write.format('parquet').mode('overwrite').save(stage2 + '/m365/RefDefinition')\n",
        "# Section\n",
        "df = spark.read.csv(stage1_m365 + '/Roster/Section.csv', header='false')\n",
        "if (df.count() > 0):\n",
        "  sqlContext.registerDataFrameAsTable(df, 'Section')\n",
        "  df = spark.sql(\"select _c0 Id, _c1 Name, _c2 Code, _c3 Location, _c4 ExternalId, to_timestamp(_c5, 'MM/dd/yyyy hh:mm:ss a') CreateDate, to_timestamp(_c6, 'MM/dd/yyyy hh:mm:ss a') LastModifiedDate, cast(_c7 as boolean) IsActive, _c8 CourseId, _c9 RefSectionTypeId, _c10 SessionId, _c11 OrgId from Section\")\n",
        "  df.write.format('parquet').mode('overwrite').save(stage2 + '/m365/Section')\n",
        "# Session\n",
        "df = spark.read.csv(stage1_m365 + '/Roster/Session.csv', header='false')\n",
        "if (df.count() > 0):\n",
        "  sqlContext.registerDataFrameAsTable(df, 'Session')\n",
        "  df = spark.sql(\"select _c0 Id, _c1 Name, to_timestamp(_c2, 'MM/dd/yyyy hh:mm:ss a') BeginDate, to_timestamp(_c3, 'MM/dd/yyyy hh:mm:ss a') EndDate, _c4 ExternalId, to_timestamp(_c5, 'MM/dd/yyyy hh:mm:ss a') CreateDate, to_timestamp(_c6, 'MM/dd/yyyy hh:mm:ss a') LastModifiedDate, cast(_c7 as boolean) IsActive, _c8 CalendarId, _c9 ParentSessionId, _c10 RefSessionTypeId from Session\")\n",
        "  df.write.format('parquet').mode('overwrite').save(stage2 + '/m365/Session')\n",
        "# StaffOrgAffiliation\n",
        "df = spark.read.csv(stage1_m365 + '/Roster/StaffOrgAffiliation.csv', header='false')\n",
        "if (df.count() > 0):\n",
        "  sqlContext.registerDataFrameAsTable(df, 'StaffOrgAffiliation')\n",
        "  df = spark.sql(\"select _c0 Id, cast(_c1 as boolean) IsPrimary, to_timestamp(_c2, 'MM/dd/yyyy hh:mm:ss a') EntryDate, to_timestamp(_c3, 'MM/dd/yyyy hh:mm:ss a') ExitDate, _c4 ExternalId, to_timestamp(_c5, 'MM/dd/yyyy hh:mm:ss a') CreateDate, to_timestamp(_c6, 'MM/dd/yyyy hh:mm:ss a') LastModifiedDate, cast(_c7 as boolean) IsActive, _c8 OrgId, _c9 PersonId, _c10 RefStaffOrgRoleId from StaffOrgAffiliation\")\n",
        "  df.write.format('parquet').mode('overwrite').save(stage2 + '/m365/StaffOrgAffiliation')\n",
        "# StaffSectionMembership\n",
        "df = spark.read.csv(stage1_m365 + '/Roster/StaffSectionMembership.csv', header='false')\n",
        "if (df.count() > 0):\n",
        "  sqlContext.registerDataFrameAsTable(df, 'StaffSectionMembership')\n",
        "  df = spark.sql(\"select _c0 Id, cast(_c1 as boolean) IsPrimaryStaffForSection, to_timestamp(_c2, 'MM/dd/yyyy hh:mm:ss a') EntryDate, to_timestamp(_c3, 'MM/dd/yyyy hh:mm:ss a') ExitDate, _c4 ExternalId, to_timestamp(_c5, 'MM/dd/yyyy hh:mm:ss a') CreateDate, to_timestamp(_c6, 'MM/dd/yyyy hh:mm:ss a') LastModifiedDate, cast(_c7 as boolean) IsActive, _c8 PersonId, _c9 RefStaffSectionRoleId, _c10 SectionId from StaffSectionMembership\")\n",
        "  df.write.format('parquet').mode('overwrite').save(stage2 + '/m365/StaffSectionMembership')\n",
        "# StudentOrgAffiliation\n",
        "df = spark.read.csv(stage1_m365 + '/Roster/StudentOrgAffiliation.csv', header='false')\n",
        "if (df.count() > 0):\n",
        "  sqlContext.registerDataFrameAsTable(df, 'StudentOrgAffiliation')\n",
        "  df = spark.sql(\"select _c0 Id, cast(_c1 as boolean) IsPrimary, to_timestamp(_c2, 'MM/dd/yyyy hh:mm:ss a') EntryDate, to_timestamp(_c3, 'MM/dd/yyyy hh:mm:ss a') ExitDate, _c4 ExternalId, to_timestamp(_c5, 'MM/dd/yyyy hh:mm:ss a') CreateDate, to_timestamp(_c6, 'MM/dd/yyyy hh:mm:ss a') LastModifiedDate, cast(_c7 as boolean) IsActive, _c8 OrgId, _c9 PersonId, _c10 RefGradeLevelId, _c11 RefStudentOrgRoleId, _c12 RefEnrollmentStatusId from StudentOrgAffiliation\")\n",
        "  df.write.format('parquet').mode('overwrite').save(stage2 + '/m365/StudentOrgAffiliation')\n",
        "# StudentSectionMembership\n",
        "df = spark.read.csv(stage1_m365 + '/Roster/StudentSectionMembership.csv', header='false')\n",
        "if (df.count() > 0):\n",
        "  sqlContext.registerDataFrameAsTable(df, 'StudentSectionMembership')\n",
        "  df = spark.sql(\"select _c0 Id, to_timestamp(_c1, 'MM/dd/yyyy hh:mm:ss a') EntryDate, to_timestamp(_c2, 'MM/dd/yyyy hh:mm:ss a') ExitDate, _c3 ExternalId, to_timestamp(_c4, 'MM/dd/yyyy hh:mm:ss a') CreateDate, to_timestamp(_c5, 'MM/dd/yyyy hh:mm:ss a') LastModifiedDate, cast(_c6 as boolean) IsActive, _c7 PersonId, _c8 RefGradeLevelWhenCourseTakenId, _c9 RefStudentSectionRoleId, _c10 SectionId from StudentSectionMembership\")\n",
        "  df.write.format('parquet').mode('overwrite').save(stage2 + '/m365/StudentSectionMembership')\n",
        "\n",
        "\n",
        "\n",
        "# Process Activity data from stage1 into stage2.\n",
        "#\n",
        "# If this is the first load, it loads all activity data.\n",
        "# If this is a subsequent load, it determines the max date currently stored and only loads data from after that date.\n",
        "\n",
        "def append_to_activity_table(max_date=False):\n",
        "    df = spark.read.csv(stage1_m365_activity, header='false') \n",
        "    sqlContext.registerDataFrameAsTable(df, 'Activity')\n",
        "    df_Activity = spark.sql(\"select to_timestamp(_c0) BinDate, _c1 Upn, _c2 UserId, _c3 Application, cast(_c4 as int) SumNumberOfSignals, _c5 Client, cast(_c6 as int) Duration, _c7 LearningActivity, '' PersonId from Activity\")\n",
        "    \n",
        "    if (max_date):\n",
        "        df_Activity = df_Activity.filter(df_Activity.BinDate > max_date)\n",
        "\n",
        "    if (df_Activity.count() == 0):\n",
        "        print('No new activity data to load')\n",
        "    else:\n",
        "        print('Adding activity data later than: ' + str(max_date))\n",
        "        # The assumption here is that there will always be data in these inbound files\n",
        "        sqlContext.registerDataFrameAsTable(df_Activity, 'Activity')\n",
        "        sqlContext.registerDataFrameAsTable(spark.read.format('parquet').load(f'{stage2}/m365/PersonIdentifier'), 'PersonIdentifier')\n",
        "        sqlContext.registerDataFrameAsTable(spark.read.format('parquet').load(f'{stage2}/m365/RefDefinition'), 'RefDefinition')\n",
        "\n",
        "        df1 = spark.sql( \\\n",
        "        \"select act.BinDate, act.Upn, act.UserId, act.Application, act.SumNumberOfSignals, act.Client, act.Duration, act.LearningActivity, pi.PersonId \\\n",
        "        from PersonIdentifier pi, RefDefinition rd, Activity act \\\n",
        "        where \\\n",
        "            pi.RefIdentifierTypeId = rd.Id \\\n",
        "            and rd.RefType = 'RefIdentifierType' \\\n",
        "            and rd.Code = 'username' \\\n",
        "            and pi.Identifier = act.Upn \\\n",
        "            and act.Upn <> '' \\\n",
        "        \")\n",
        "\n",
        "        df2 = spark.sql( \\\n",
        "        \"select act.BinDate, act.Upn, act.UserId, act.Application, act.SumNumberOfSignals, act.Client, act.Duration, act.LearningActivity, pi.PersonId \\\n",
        "        from PersonIdentifier pi, RefDefinition rd, Activity act\\\n",
        "        where \\\n",
        "            pi.RefIdentifierTypeId = rd.Id\\\n",
        "            and rd.RefType = 'RefIdentifierType'\\\n",
        "            and rd.Code = 'ActiveDirectoryId'\\\n",
        "            and pi.Identifier = act.UserId\\\n",
        "            and act.UserId is not null\\\n",
        "        \")\n",
        "\n",
        "        df1.write.format(\"parquet\").mode(\"append\").save(f'{stage2}/m365/Activity')\n",
        "        df2.write.format(\"parquet\").mode(\"append\").save(f'{stage2}/m365/Activity')\n",
        "\n",
        "try:\n",
        "    df = spark.read.format('parquet').load(f'{stage2}/m365/Activity')\n",
        "    max_date = df.agg({'BinDate': 'max'}).first()[0]\n",
        "    print(max_date)\n",
        "    append_to_activity_table(max_date)\n",
        "except:\n",
        "    print(\"No Activity data has been loaded into stage2 data lake yet.\")\n",
        "    append_to_activity_table()\n",
        "\n",
        "\n",
        "\n",
        "# Anonymize the data from edu_dl (stage2) and load into anon_edu_dl (stage3)\n",
        "# - redact columns from Person table\n",
        "# - apply a hash to every occurrence of PersonId\n",
        "# - redact UPN and UserId from Activity table\n",
        "# - don't bring in PersonIdentifier, Student, Staff and other tables not needed or empty (some tables are not being populated by EDP)\n",
        "\n",
        "from pyspark.sql.functions import sha2, lit\n",
        "# Activity\n",
        "df = spark.read.format('parquet').load(f'{stage2}/m365/Activity')\n",
        "df = df.withColumn('PersonId', sha2(df.PersonId, 256)).withColumn('Upn', lit('*')).withColumn('UserId', lit('*'))\n",
        "df.write.format('parquet').mode('overwrite').save(f'{stage3}/m365/Activity')\n",
        "# Calendar, Course, Org\n",
        "spark.read.format('parquet').load(f'{stage2}/m365/Calendar').write.format('parquet').mode('overwrite').save(f'{stage3}/m365/Calendar')\n",
        "spark.read.format('parquet').load(f'{stage2}/m365/Course').write.format('parquet').mode('overwrite').save(f'{stage3}/m365/Course')\n",
        "spark.read.format('parquet').load(f'{stage2}/m365/Org').write.format('parquet').mode('overwrite').save(f'{stage3}/m365/Org')\n",
        "# Person\n",
        "df = spark.read.format('parquet').load(f'{stage2}/m365/Person')\n",
        "df = df.withColumn('Id', sha2(df.Id, 256)).withColumn('FirstName', lit('*')).withColumn(\"MiddleName\", lit('*')).withColumn('LastName', lit('*')).withColumn('ExternalId', sha2(df.ExternalId, 256))\n",
        "df.write.format('parquet').mode('overwrite').save(f'{stage3}/m365/Person')\n",
        "# PersonIdentifier\n",
        "df = spark.read.format('parquet').load(f'{stage2}/m365/PersonIdentifier')\n",
        "df = df.withColumn('PersonId', sha2(df.Id, 256)).withColumn('Identifier', lit('*')).withColumn(\"ExternalId\", lit('*'))\n",
        "df.write.format('parquet').mode('overwrite').save(f'{stage3}/m365/PersonIdentifier')\n",
        "# RefDefinition, Section, Session\n",
        "spark.read.format('parquet').load(f'{stage2}/m365/RefDefinition').write.format('parquet').mode('overwrite').save(f'{stage3}/m365/RefDefinition')\n",
        "spark.read.format('parquet').load(f'{stage2}/m365/Section').write.format('parquet').mode('overwrite').save(f'{stage3}/m365/Section')\n",
        "spark.read.format('parquet').load(f'{stage2}/m365/Session').write.format('parquet').mode('overwrite').save(f'{stage3}/m365/Session')\n",
        "# StaffOrgAffiliation\n",
        "df = spark.read.format('parquet').load(f'{stage2}/m365/StaffOrgAffiliation')\n",
        "df = df.withColumn('PersonId', sha2(df.PersonId, 256)).withColumn('ExternalId', lit('*'))\n",
        "df.write.format('parquet').mode('overwrite').save(f'{stage3}/m365/StaffOrgAffiliation')\n",
        "# StaffSectionMembership\n",
        "df = spark.read.format('parquet').load(f'{stage2}/m365/StaffSectionMembership')\n",
        "df = df.withColumn('PersonId', sha2(df.PersonId, 256)).withColumn('ExternalId', lit('*'))\n",
        "df.write.format('parquet').mode('overwrite').save(f'{stage3}/m365/StaffSectionMembership')\n",
        "# StudentOrgAffiliation\n",
        "df = spark.read.format('parquet').load(f'{stage2}/m365/StudentOrgAffiliation')\n",
        "df = df.withColumn('PersonId', sha2(df.PersonId, 256)).withColumn('ExternalId', lit('*'))\n",
        "df.write.format('parquet').mode('overwrite').save(f'{stage3}/m365/StudentOrgAffiliation')\n",
        "# StudentSectionMembership\n",
        "df = spark.read.format('parquet').load(f'{stage2}/m365/StudentSectionMembership')\n",
        "df = df.withColumn('PersonId', sha2(df.PersonId, 256)).withColumn('ExternalId', lit('*'))\n",
        "df.write.format('parquet').mode('overwrite').save(f'{stage3}/m365/StudentSectionMembership')\n",
        "\n",
        "\n",
        "# Create spark db to allow for access to the data in the delta-lake via SQL on-demand.\n",
        "# This is only creating metadata for SQL on-demand, pointing to the data in the delta-lake.\n",
        "# This also makes it possible to connect in Power BI via the azure sql data source connector.\n",
        "def create_spark_db(db_name, source_path):\n",
        "    spark.sql('CREATE DATABASE IF NOT EXISTS ' + db_name)\n",
        "    spark.sql(\"create table if not exists \" + db_name + \".Activity using PARQUET location '\" + source_path + \"/Activity'\")\n",
        "    spark.sql(\"create table if not exists \" + db_name + \".Calendar using PARQUET location '\" + source_path + \"/Calendar'\")\n",
        "    spark.sql(\"create table if not exists \" + db_name + \".Course using PARQUET location '\" + source_path + \"/Course'\")\n",
        "    spark.sql(\"create table if not exists \" + db_name + \".Org using PARQUET location '\" + source_path + \"/Org'\")\n",
        "    spark.sql(\"create table if not exists \" + db_name + \".Person using PARQUET location '\" + source_path + \"/Person'\")\n",
        "    spark.sql(\"create table if not exists \" + db_name + \".PersonIdentifier using PARQUET location '\" + source_path + \"/PersonIdentifier'\")\n",
        "    spark.sql(\"create table if not exists \" + db_name + \".RefDefinition using PARQUET location '\" + source_path + \"/RefDefinition'\")\n",
        "    spark.sql(\"create table if not exists \" + db_name + \".Section using PARQUET location '\" + source_path + \"/Section'\")\n",
        "    spark.sql(\"create table if not exists \" + db_name + \".Session using PARQUET location '\" + source_path + \"/Session'\")\n",
        "    spark.sql(\"create table if not exists \" + db_name + \".StaffOrgAffiliation using PARQUET location '\" + source_path + \"/StaffOrgAffiliation'\")\n",
        "    spark.sql(\"create table if not exists \" + db_name + \".StaffSectionMembership using PARQUET location '\" + source_path + \"/StaffSectionMembership'\")\n",
        "    spark.sql(\"create table if not exists \" + db_name + \".StudentOrgAffiliation using PARQUET location '\" + source_path + \"/StudentOrgAffiliation'\")\n",
        "    spark.sql(\"create table if not exists \" + db_name + \".StudentSectionMembership using PARQUET location '\" + source_path + \"/StudentSectionMembership'\")\n",
        "\n",
        "db_prefix = 'test_' if use_test_env else ''\n",
        "create_spark_db(db_prefix + 's2_m365', stage2 + '/m365')\n",
        "create_spark_db(db_prefix + 's3_m365', stage3 + '/m365')"
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "outputs": [],
      "metadata": {},
      "source": [
        "# Extracted from Contoso_ISD_setup\n",
        "from pyspark.sql.functions import sha2, lit\n",
        "\n",
        "# Process studentsectionmark and studentattendance\n",
        "df = spark.read.csv(stage1 + '/contoso_sis/studentsectionmark.csv', header='true', inferSchema='true')\n",
        "df = df.withColumn('id',df.id.cast('string')).withColumn('student_id',df.student_id.cast('string'))\n",
        "df.write.format('parquet').mode('overwrite').save(stage2 + '/contoso_sis/studentsectionmark')\n",
        "\n",
        "df = spark.read.csv(stage1 + '/contoso_sis/studentattendance.csv', header='true', inferSchema='true')\n",
        "df = df.withColumn('id',df.id.cast('string')).withColumn('student_id',df.student_id.cast('string'))\n",
        "df.write.format('parquet').mode('overwrite').save(stage2 + '/contoso_sis/studentattendance')\n",
        "\n",
        "# Anonymize data and load into stage3\n",
        "df = spark.read.format('parquet').load(stage2 + '/contoso_sis/studentsectionmark')\n",
        "df = df.withColumn('id', sha2(df.id, 256)).withColumn('student_id',sha2(df.student_id, 256))\n",
        "df.write.format('parquet').mode('overwrite').save(stage3 + '/contoso_sis/studentsectionmark')\n",
        "\n",
        "df = spark.read.format('parquet').load(stage2 + '/contoso_sis/studentattendance')\n",
        "df = df.withColumn('id', sha2(df.id, 256)).withColumn('student_id',sha2(df.student_id, 256))\n",
        "df.write.format('parquet').mode('overwrite').save(stage3 + '/contoso_sis/studentattendance')\n",
        "\n",
        "# Create spark db to allow for access to the data in the delta-lake via SQL on-demand.\n",
        "def create_spark_db(db_name, source_path):\n",
        "    spark.sql('CREATE DATABASE IF NOT EXISTS ' + db_name)\n",
        "    spark.sql(\"create table if not exists \" + db_name + \".studentsectionmark using PARQUET location '\" + source_path + \"/studentsectionmark'\")\n",
        "    spark.sql(\"create table if not exists \" + db_name + \".studentattendance using PARQUET location '\" + source_path + \"/studentattendance'\")\n",
        "\n",
        "db_prefix = 'test_' if use_test_env else ''\n",
        "create_spark_db(db_prefix + 's2_contoso_sis', stage2 + '/contoso_sis')\n",
        "create_spark_db(db_prefix + 's3_contoso_sis', stage3 + '/contoso_sis')"
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "outputs": [],
      "metadata": {},
      "source": [
        "# Extracted from Contoso_ISD_setup_and_update\n",
        "\n",
        "# Process sectionmark data\n",
        "# Convert id values to use the Person.Id and Section.Id values set in the Education Data Platform.\n",
        "from pyspark.sql.functions import sha2, lit\n",
        "\n",
        "sqlContext.registerDataFrameAsTable(spark.read.format('parquet').load(stage2 + '/contoso_sis/studentsectionmark'), 'SectionMark')\n",
        "\n",
        "sqlContext.registerDataFrameAsTable(spark.read.format('parquet').load(stage2 + '/m365/Person'), 'Person')\n",
        "sqlContext.registerDataFrameAsTable(spark.read.format('parquet').load(stage2 + '/m365/Section'), 'Section')\n",
        "\n",
        "df = spark.sql(\"select sm.id Id, p.Id PersonId, s.Id SectionId, cast(sm.numeric_grade_earned as int) NumericGrade, \\\n",
        "sm.alpha_grade_earned AlphaGrade, sm.is_final_grade IsFinalGrade, cast(sm.credits_attempted as int) CreditsAttempted, cast(sm.credits_earned as int) CreditsEarned, \\\n",
        "sm.grad_credit_type GraduationCreditType, sm.id ExternalId, CURRENT_TIMESTAMP CreateDate, CURRENT_TIMESTAMP LastModifiedDate, true IsActive \\\n",
        "from SectionMark sm, Person p, Section s \\\n",
        "where sm.student_id = p.ExternalId \\\n",
        "and sm.section_id = s.ExternalId\")\n",
        "\n",
        "df.write.format('parquet').mode('overwrite').save(stage2 + '/ContosoISD/SectionMark')\n",
        "df.write.format('parquet').mode('overwrite').save(stage2 + '/ContosoISD/SectionMark2')\n",
        "# Add SectionMark data to stage3 (anonymized parquet lake)\n",
        "df = df.withColumn('PersonId', sha2(df.PersonId, 256))\n",
        "df.write.format('parquet').mode('overwrite').save(stage3 + '/ContosoISD/SectionMark')\n",
        "df.write.format('parquet').mode('overwrite').save(stage3 + '/ContosoISD/SectionMark2')\n",
        "\n",
        "# Repeat the above process, this time for student attendance\n",
        "# Convert id values to use the Person.Id, Org.Id and Section.Id values\n",
        "sqlContext.registerDataFrameAsTable(spark.read.format('parquet').load(stage2 + '/contoso_sis/studentattendance'), 'Attendance')\n",
        "sqlContext.registerDataFrameAsTable(spark.read.format('parquet').load(stage2 + '/m365/Org'), 'Org')\n",
        "\n",
        "df = spark.sql(\"select att.id Id, p.Id PersonId, att.school_year SchoolYear, o.Id OrgId, to_date(att.attendance_date,'MM/dd/yyyy') AttendanceDate, \\\n",
        "att.all_day AllDay, att.Period Period, s.Id SectionId, att.AttendanceCode AttendanceCode, att.PresenceFlag PresenceFlag, \\\n",
        "att.attendance_status AttendanceStatus, att.attendance_type AttendanceType, att.attendance_sequence AttendanceSequence \\\n",
        "from Attendance att, Org o, Person p, Section s \\\n",
        "where att.student_id = p.ExternalId \\\n",
        "and att.school_id = o.ExternalId \\\n",
        "and att.section_id = s.ExternalId\")\n",
        "\n",
        "df.write.format('parquet').mode('overwrite').save(stage2 +'/ContosoISD/Attendance')\n",
        "# Add Attendance data to stage3 (anonymized parquet lake)\n",
        "df = df.withColumn('PersonId', sha2(df.PersonId, 256))\n",
        "df.write.format('parquet').mode('overwrite').save(stage3 + '/ContosoISD/Attendance')\n",
        "\n",
        "# Add 'Department' column to Course (hardcoded to \"Math\" for this Contoso example)\n",
        "sqlContext.registerDataFrameAsTable(spark.read.format('parquet').load(stage2 + '/m365/Course'), 'Course')\n",
        "df = spark.sql(\"select Id, Name, Code, Description, ExternalId, CreateDate, LastModifiedDate, IsActive, CalendarId, 'Math' Department from Course\")\n",
        "df.write.format('parquet').mode('overwrite').save(stage2 + '/ContosoISD/Course')\n",
        "df.write.format('parquet').mode('overwrite').save(stage3 + '/ContosoISD/Course')\n",
        "\n",
        "# Create spark db to allow for access to the data in the delta-lake via SQL on-demand.\n",
        "# This is only creating metadata for SQL on-demand, pointing to the data in the delta-lake.\n",
        "# This also makes it possible to connect in Power BI via the azure sql data source connector.\n",
        "def create_spark_db(db_name, source_path):\n",
        "    spark.sql('CREATE DATABASE IF NOT EXISTS ' + db_name)\n",
        "    spark.sql(f\"create table if not exists \" + db_name + \".Activity using PARQUET location '\" + source_path + \"/m365/Activity'\")\n",
        "    spark.sql(f\"create table if not exists \" + db_name + \".Calendar using PARQUET location '\" + source_path + \"/m365/Calendar'\")\n",
        "    spark.sql(f\"create table if not exists \" + db_name + \".Org using PARQUET location '\" + source_path + \"/m365/Org'\")\n",
        "    spark.sql(f\"create table if not exists \" + db_name + \".Person using PARQUET location '\" + source_path + \"/m365/Person'\")\n",
        "    spark.sql(f\"create table if not exists \" + db_name + \".PersonIdentifier using PARQUET location '\" + source_path + \"/m365/PersonIdentifier'\")\n",
        "    spark.sql(f\"create table if not exists \" + db_name + \".RefDefinition using PARQUET location '\" + source_path + \"/m365/RefDefinition'\")\n",
        "    spark.sql(f\"create table if not exists \" + db_name + \".Section using PARQUET location '\" + source_path + \"/m365/Section'\")\n",
        "    spark.sql(f\"create table if not exists \" + db_name + \".Session using PARQUET location '\" + source_path + \"/m365/Session'\")\n",
        "    spark.sql(f\"create table if not exists \" + db_name + \".StaffOrgAffiliation using PARQUET location '\" + source_path + \"/m365/StaffOrgAffiliation'\")\n",
        "    spark.sql(f\"create table if not exists \" + db_name + \".StaffSectionMembership using PARQUET location '\" + source_path + \"/m365/StaffSectionMembership'\")\n",
        "    spark.sql(f\"create table if not exists \" + db_name + \".StudentOrgAffiliation using PARQUET location '\" + source_path + \"/m365/StudentOrgAffiliation'\")\n",
        "    spark.sql(f\"create table if not exists \" + db_name + \".StudentSectionMembership using PARQUET location '\" + source_path + \"/m365/StudentSectionMembership'\")\n",
        "\n",
        "    spark.sql(f\"create table if not exists \" + db_name + \".Course using PARQUET location '\" + source_path + \"/ContosoISD/Course'\")\n",
        "    spark.sql(f\"create table if not exists \" + db_name + \".Attendance using PARQUET location '\" + source_path + \"/ContosoISD/Attendance'\")\n",
        "    spark.sql(f\"create table if not exists \" + db_name + \".SectionMark using PARQUET location '\" + source_path + \"/ContosoISD/SectionMark'\")\n",
        "    spark.sql(f\"create table if not exists \" + db_name + \".SectionMark2 using PARQUET location '\" + source_path + \"/ContosoISD/SectionMark2'\")\n",
        "\n",
        "db_prefix = 'test_' if use_test_env else ''\n",
        "create_spark_db(db_prefix + 's2_ContosoISD', stage2)\n",
        "create_spark_db(db_prefix + 's3_ContosoISD', stage3)"
      ],
      "attachments": {}
    }
  ]
}